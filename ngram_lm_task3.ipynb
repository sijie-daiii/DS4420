{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 2: n-gram LM - Task 3\n",
    "----\n",
    "\n",
    "See the `task1` notebook for instructions. Include *all* AI and other citations in the `task4` notebook and in line as appropriate.\n",
    "\n",
    "Name: __Sijie Dai__\n",
    "\n",
    "**IMPORTANT: If you edit the lm_model.py file, restart the kernel after each edit. Otherwise you won't see the changes in the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is the majority of the work for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Implement an n-gram Language Model (`lm_model.py` 60 points, other code 17.5 points)\n",
    "----\n",
    "\n",
    "__Start__ by implementing the `LanguageModel` class as outlined in the provided `lm_starter.py` file. Do not change function signatures (the unit tests that we provide and the unit tests in the autograder will break if you do).\n",
    "\n",
    "Your language model:\n",
    "- *must* work for both the unigram and bigram cases\n",
    "- for `n` values larger than 2: 5 points are allocated to an experiment that tests the generalizability of your model\n",
    "    - hint: try to implement the bigram case as a generalized \"`n` is greater than 1\" case\n",
    "- *must* be *token agnostic* (this means that if we give the model text tokenized as single characters, it will function as a character language model but if we give the model text tokenized as \"words\" (or \"traditionally\"), then it will function as a language model with those tokens)\n",
    "- uses Laplace smoothing *for scoring*\n",
    "- replaces all tokens that occur *only once* with `<UNK>` at train time\n",
    "    - note: do not add `<UNK>` to your vocabulary if no tokens in the training data occur only once!\n",
    "- does *not* use Laplace smoothing *for generation* \n",
    "\n",
    "We have provided:\n",
    "- a function to read in files\n",
    "- some functions to change a list of strings into tokens (that you can run to tokenize by \"word\" or by character)\n",
    "- the skeleton of the `LanguageModel` class\n",
    "\n",
    "You need to implement:\n",
    "- all functions marked in `lm_starter.py`, including optional parameters, where they exist\n",
    "\n",
    "You may implement:\n",
    "- additional functions/methods as helpful to you\n",
    "\n",
    "As a guideline, including comments, all empty lines, all code required and some debugging code that can be run with `verbose` parameters, our solution is ~285 lines. (~+105 lines versus the starter code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename your lm_starter.py file to lm_model.py and put it in the same directory as this file\n",
    "# to import into this notebook\n",
    "\n",
    "# IMPORTANT: whenever you make changes to lm_model.py, you must RESTART the kernel in this notebook\n",
    "# otherwise you'll be working with stale code!\n",
    "import lm_model as lm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the language model (unit tests)\n",
    "-----\n",
    "\n",
    "Do this __*from the command line*__.\n",
    "\n",
    "Passing all provided unit tests is a good indication that your model is correct. They are *not a guarantee*, so make sure to look at the tests and the cases that they cover. (we'll be testing your model against all of the testing data in addition).\n",
    "\n",
    "The autograder points in gradescope are assigned __50 points__. There are __an additional 10 points__ manually graded for the correctness of your sentence generation.\n",
    "\n",
    "To run tests: make sure all training files are in a `training_files` directory that is in the same directory as this notebook.\n",
    "\n",
    "Instructions for Running Unit Tests from the command line:\n",
    "- To run the unit tests, navigate to the directory where your code files are located and execute the following command:\n",
    "`python -m unittest test_file_name`\n",
    "    - Ensure that your Python environment has access to the necessary dependencies and that all modules are properly imported.\n",
    "    - come to office hours/post on the course discussion board with any installation issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrate using your model\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1: don't russian\n",
      " 2: to would go i about have sundays on\n",
      " 3: want more\n",
      " 4: help\n",
      " 5: \n",
      " 6: particularly in\n",
      " 7: you in try care\n",
      " 8: \n",
      " 9: barbecue\n",
      "10: can i'd i be over\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# instantiate a unigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 1\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = True\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "np.random.seed(42)\n",
    "tokens_words = lm.tokenize(data, ngram, by_char=False)\n",
    "\n",
    "lm_model = lm.LanguageModel(ngram)\n",
    "lm_model.train(tokens_words)\n",
    "\n",
    "for i, sent in enumerate(lm_model.generate(10), 1):\n",
    "    pretty = ' '.join(tok for tok in sent\n",
    "                      if tok not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END))\n",
    "    print(f\"{i:2d}: {pretty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level bigram samples:\n",
      "\n",
      " 1: i ka sd\n",
      " 2: llood y ge i iteau-chrild ikeyof wan farg t sheatar to he fouhi ssthed aicau m wi vetofferautonenean fould livinere d\n",
      " 3: t\n",
      " 4: g t g\n",
      " 5: uh wacherd u gooullpis\n",
      " 6: permaut tesho bantiod erabe t okikexivet re youicsife anthio'thatoouteabrseabe betean\n",
      " 7: ue aingo r\n",
      " 8: is aran at o wouve todos\n",
      " 9: ikfos ingofo uhestont tt warmienngod wikauhe chives t i't ccobo t st turan li'liautom\n",
      "10: llit\n"
     ]
    }
   ],
   "source": [
    "# instantiate a bigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 2\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = True\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Expected runtime close to 0.1 sec\n",
    "\n",
    "# Train the language model\n",
    "lm_char_bigram = lm.LanguageModel(ngram)\n",
    "lm_char_bigram.train(tokens)\n",
    "\n",
    "print(\"Character-level bigram samples:\\n\")\n",
    "for i in range(1, 11):\n",
    "    seq = lm_char_bigram.generate_sentence()\n",
    "    pretty = ''.join(ch for ch in seq\n",
    "                     if ch not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END))\n",
    "    print(f\"{i:2d}: {pretty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: WORD-level\n",
      "Mean score: 4.962082362726267e-05\n",
      "Standard deviation of score: 0.000286735365135695\n"
     ]
    }
   ],
   "source": [
    "# evaluate a **word-based bigram model** on the test data\n",
    "# make sure to use the correct model!\n",
    "\n",
    "# score each line in the test data individually, then calculate the average score\n",
    "# you need not re-train your model, just be sure to use the correct model!\n",
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file(test_path)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from statistics import mean, stdev\n",
    "\n",
    "ngram   = 2\n",
    "by_char = False   \n",
    "train_path   = \"training_files/berp-training.txt\"\n",
    "train_tokens = lm.tokenize(lm.read_file(train_path),\n",
    "                           ngram, by_char=by_char)\n",
    "model = lm.LanguageModel(ngram)\n",
    "model.train(train_tokens)\n",
    "\n",
    "# score every sentence in the test set \n",
    "for line in test_data:\n",
    "    line = line.strip()\n",
    "    if not line:            \n",
    "        continue\n",
    "    sent_tokens = lm.tokenize_line(line, ngram, by_char=by_char)\n",
    "    scores.append(model.score(sent_tokens))\n",
    "\n",
    "μ = mean(scores)\n",
    "σ = stdev(scores)\n",
    "\n",
    "print(\"Tokenization: WORD-level\")\n",
    "print(f\"Mean score: {μ}\")\n",
    "print(f\"Standard deviation of score: {σ}\")\n",
    "# Expected runtime close to 0.1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 points\n",
    "\n",
    "# Find a new data set and run some training experiments\n",
    "# see if you can train your model on a data other than the provided data sets\n",
    "\n",
    "# Find another dataset on your own that fits following parameters and verify that you can run the language model on it for different values of n. \n",
    "# your file must:\n",
    "# - contain a minimum of 10,000 tokens (tokenized by \"words\") or 100,000 tokens (tokenized by characters)\n",
    "# - be in a text format\n",
    "# - not be restaurant reviews\n",
    "\n",
    "# your file need not (we encourage you to explore here):\n",
    "# - be in English\n",
    "\n",
    "# You must submit your data file to Gradescope along with `lm_model.py` and `ngram_lm.ipynb`.\n",
    "# if your data file is too large, you can use the `head` command to get a smaller version of it\n",
    "# please submit the smaller version of your data file *as well as* a zip file of the original data file\n",
    "\n",
    "# Here are some resources for finding datasets:\n",
    "# - https://www.gutenberg.org (public domain books)\n",
    "# - https://www.kaggle.com/datasets (you may have to manually convert your dataset into text.)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# First, print some attributes of your data set.\n",
    "# How big is it, how many unique tokens, etc.\n",
    "# Print an example text snippet from your data set.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Next, train your model on your data set for n = 1, 2, 3, 4, 5, ... 10\n",
    "# For each value of n, print the training time and generate three example sentences\n",
    "# stop if the training time exceeds 3 minutes, even if you haven't reached n = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14,533 raw lines from pride_prejudice.txt\n",
      "Total word tokens : 150,525\n",
      "Unique word types : 14,162\n",
      "\n",
      "--- sample snippet ---------------------------------------------\n",
      "harge struck her too forcibly  for denial; and the circumstances to which he\n",
      "particularly alluded, as  having passed at the Netherfield ball, and as\n",
      "confirming all his first  disapprobation, could not have made a stronger\n",
      "impression on his mind  than on hers.    The compliment to herself and her sis\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "===== n-gram experiments (word level) ==========================\n",
      "\n",
      "[n = 1] training time = 0.1 s, V = 6,144\n",
      "  1. \n",
      "  2. is <UNK> <UNK> every would to better be pounds, afterwards had\n",
      "  3. \n",
      "\n",
      "[n = 2] training time = 0.1 s, V = 6,144\n",
      "  1. last letter from _hauteur_ or at\n",
      "  2. disgraceful companions. that her husband, who has ever to do not account that his resemblance of its <UNK> from the report, which might be mistaken. on us, and, above\n",
      "  3. very early, somewhere near her, had reached the elevation of her, when she <UNK>\n",
      "\n",
      "[n = 3] training time = 0.1 s, V = 6,144\n",
      "  1. to <UNK> at the next day. it was given\n",
      "  2. were <UNK>\n",
      "  3. before he had made him\n",
      "\n",
      "[n = 4] training time = 0.1 s, V = 6,144\n",
      "  1. wickham. <UNK> of wickham! every kind of pride must <UNK> from\n",
      "  2. plan, and she gradually learned to consider it as given in\n",
      "  3. the other ladies often <UNK> the gentlemen being out, they had in fact\n",
      "\n",
      "[n = 5] training time = 0.1 s, V = 6,144\n",
      "  1. always sure of leisure and tranquillity; and though <UNK> as he told\n",
      "  2. was, at any rate, a ball. and even mary could assure her family that she\n",
      "  3. very agreeable, and i give you leave to like him. you have liked many a\n",
      "\n",
      "[n = 6] training time = 0.2 s, V = 6,144\n",
      "  1. disinterested at least, for he must know my father can give her nothing.\n",
      "  2. place or other, and we can never expect her to do it with so little\n",
      "  3. sometimes coming there stopped her, and instead of entering the park,\n",
      "\n",
      "[n = 7] training time = 0.2 s, V = 6,144\n",
      "  1. himself at her <UNK> he was anxious to avoid the notice of his cousins,\n",
      "  2. which elizabeth received from jane as soon as she entered the room, and\n",
      "  3. credit. that he was really fond of jane, she doubted no more than she\n",
      "\n",
      "[n = 8] training time = 0.2 s, V = 6,144\n",
      "  1. “i hope,” added mrs. gardiner, “that no consideration with regard to\n",
      "  2. confusion, lest they had been <UNK>\n",
      "  3. was her business to be <UNK> certainly her temper to be happy;\n",
      "\n",
      "[n = 9] training time = 0.2 s, V = 6,144\n",
      "  1. me; i should not have been allowed to invite them.”\n",
      "  2. was angry that the ball closed so early, and talked of giving one\n",
      "  3. other, would have told it all. _their_ situation was awkward enough; but\n",
      "\n",
      "[n = 10] training time = 0.2 s, V = 6,144\n",
      "  1. last she was <UNK> for the truth of every particular to colonel\n",
      "  2. but here, by carrying with me one <UNK> source of regret in my\n",
      "  3. down the ridicule and censure of the ladies both of netherfield and\n"
     ]
    }
   ],
   "source": [
    "import textwrap, os, random\n",
    "\n",
    "# Example: Project-Gutenberg\n",
    "data_path = \"pride_prejudice.txt\"   \n",
    "raw_lines = lm.read_file(data_path)            \n",
    "print(f\"Loaded {len(raw_lines):,} raw lines from {data_path}\")\n",
    "\n",
    "# Quick corpus stats\n",
    "word_tokens = lm.tokenize(raw_lines, ngram=1, by_char=False)\n",
    "unique_words = set(word_tokens)\n",
    "print(f\"Total word tokens : {len(word_tokens):,}\")\n",
    "print(f\"Unique word types : {len(unique_words):,}\")\n",
    "\n",
    "# show a random snippet (≈ 300 chars)\n",
    "joined_text = \" \".join(raw_lines)\n",
    "start = random.randrange(0, max(len(joined_text) - 300, 1))\n",
    "print(\"\\n--- sample snippet ---------------------------------------------\")\n",
    "print(textwrap.fill(joined_text[start : start + 300], width=80))\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Train & sample for n = 1 … 10\n",
    "print(\"\\n===== n-gram experiments (word level) ==========================\")\n",
    "\n",
    "for n in range(1, 11):\n",
    "    t0 = time.time()\n",
    "    tokens = lm.tokenize(raw_lines, ngram=n, by_char=False)   \n",
    "    model = lm.LanguageModel(n)\n",
    "    model.train(tokens)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\n[n = {n}] training time = {elapsed:0.1f} s, V = {len(model.vocab):,}\")\n",
    "\n",
    "    # generate 3 example sentences\n",
    "    for i in range(1, 4):\n",
    "        sent = model.generate_sentence()\n",
    "        pretty = \" \".join(tok for tok in sent\n",
    "                          if tok not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END))\n",
    "        print(f\"  {i}. {pretty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP!!!\n",
    "=====\n",
    "\n",
    "Before turning any homework notebook in:\n",
    "\n",
    "- When you have finished each notebook, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that your notebook displays properly in Gradescope\n",
    "- Double check that your notebook does not display too much output (don't make us go on a treasure hunt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
