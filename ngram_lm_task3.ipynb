{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 2: n-gram LM - Task 3\n",
    "----\n",
    "\n",
    "See the `task1` notebook for instructions. Include *all* AI and other citations in the `task4` notebook and in line as appropriate.\n",
    "\n",
    "Name: __Sijie Dai__\n",
    "\n",
    "**IMPORTANT: If you edit the lm_model.py file, restart the kernel after each edit. Otherwise you won't see the changes in the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is the majority of the work for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Implement an n-gram Language Model (`lm_model.py` 60 points, other code 17.5 points)\n",
    "----\n",
    "\n",
    "__Start__ by implementing the `LanguageModel` class as outlined in the provided `lm_starter.py` file. Do not change function signatures (the unit tests that we provide and the unit tests in the autograder will break if you do).\n",
    "\n",
    "Your language model:\n",
    "- *must* work for both the unigram and bigram cases\n",
    "- for `n` values larger than 2: 5 points are allocated to an experiment that tests the generalizability of your model\n",
    "    - hint: try to implement the bigram case as a generalized \"`n` is greater than 1\" case\n",
    "- *must* be *token agnostic* (this means that if we give the model text tokenized as single characters, it will function as a character language model but if we give the model text tokenized as \"words\" (or \"traditionally\"), then it will function as a language model with those tokens)\n",
    "- uses Laplace smoothing *for scoring*\n",
    "- replaces all tokens that occur *only once* with `<UNK>` at train time\n",
    "    - note: do not add `<UNK>` to your vocabulary if no tokens in the training data occur only once!\n",
    "- does *not* use Laplace smoothing *for generation* \n",
    "\n",
    "We have provided:\n",
    "- a function to read in files\n",
    "- some functions to change a list of strings into tokens (that you can run to tokenize by \"word\" or by character)\n",
    "- the skeleton of the `LanguageModel` class\n",
    "\n",
    "You need to implement:\n",
    "- all functions marked in `lm_starter.py`, including optional parameters, where they exist\n",
    "\n",
    "You may implement:\n",
    "- additional functions/methods as helpful to you\n",
    "\n",
    "As a guideline, including comments, all empty lines, all code required and some debugging code that can be run with `verbose` parameters, our solution is ~285 lines. (~+105 lines versus the starter code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename your lm_starter.py file to lm_model.py and put it in the same directory as this file\n",
    "# to import into this notebook\n",
    "\n",
    "# IMPORTANT: whenever you make changes to lm_model.py, you must RESTART the kernel in this notebook\n",
    "# otherwise you'll be working with stale code!\n",
    "import lm_model as lm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the language model (unit tests)\n",
    "-----\n",
    "\n",
    "Do this __*from the command line*__.\n",
    "\n",
    "Passing all provided unit tests is a good indication that your model is correct. They are *not a guarantee*, so make sure to look at the tests and the cases that they cover. (we'll be testing your model against all of the testing data in addition).\n",
    "\n",
    "The autograder points in gradescope are assigned __50 points__. There are __an additional 10 points__ manually graded for the correctness of your sentence generation.\n",
    "\n",
    "To run tests: make sure all training files are in a `training_files` directory that is in the same directory as this notebook.\n",
    "\n",
    "Instructions for Running Unit Tests from the command line:\n",
    "- To run the unit tests, navigate to the directory where your code files are located and execute the following command:\n",
    "`python -m unittest test_file_name`\n",
    "    - Ensure that your Python environment has access to the necessary dependencies and that all modules are properly imported.\n",
    "    - come to office hours/post on the course discussion board with any installation issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrate using your model\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:  \n",
      " 2: odiaeoosar awofft o aoue hlaa saswaunvs \n",
      " 3:  pstpom\n",
      " 4:  u\n",
      " 5: t fau'eeholown on   np nr kminovoe   ralfudteaa h    dtgtaio snqrnh dict innaanibfiottsn p uoie otoeld e  dh ro x oeevfth\n",
      " 6: acwo oieehtnl\n",
      " 7: emosut ujegemoakuao hm eailu taaae\n",
      " 8:  enhoh\n",
      " 9: r l wuatign oeark is ni\n",
      "10: mauoti\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# instantiate a unigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 1\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = True\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "np.random.seed(42)                \n",
    "\n",
    "lm_model = lm.LanguageModel(ngram)\n",
    "lm_model.train(tokens)\n",
    "\n",
    "# Generate & pretty-print ten sentences\n",
    "for i, sent in enumerate(lm_model.generate(10), 1):\n",
    "    core   = [tok for tok in sent                                  \n",
    "              if tok not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END)]\n",
    "    pretty = ''.join(core) if by_char else ' '.join(core)          \n",
    "    print(f\"{i:2d}: {pretty}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level bigram samples:\n",
      "\n",
      " 1: i d ma n'd e goderener forentave wir u t me t ou-kst f o g thar g-kesdoranturmi's foligayo ge to bow winny liatworesay sizzes\n",
      " 2: th and\n",
      " 3: iny ry t lim pasp i ay sh tewenganaf t'soon waurelyp t tist'ty\n",
      " 4: ik nt t t statr\n",
      " 5: woouterrabrint doland\n",
      " 6: p__mokereranu te it her aro g ve ine au wanesstortur lesireray wivenne lunt haiay iker huray f ch ch\n",
      " 7: i t cay dis fisoula tica f an lizat's teantafe e rareray o ot atanowikicetue bokapen rees\n",
      " 8: gesi'd cthay li'st bollikewhen mengho atwai i buhe mourt\n",
      " 9: st ke oreaullifutangann fi meshi tikelfol wo'domit mal il ba wi d he o ite\n",
      "10: ike t erasteneauthamo f t h hof ttoddoube brspost oos me midand ngod\n"
     ]
    }
   ],
   "source": [
    "# instantiate a bigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 2\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = True\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Expected runtime close to 0.1 sec\n",
    "\n",
    "# Train the language model\n",
    "lm_char_bigram = lm.LanguageModel(ngram)\n",
    "lm_char_bigram.train(tokens)\n",
    "\n",
    "print(\"Character-level bigram samples:\\n\")\n",
    "for i in range(1, 11):\n",
    "    seq = lm_char_bigram.generate_sentence()\n",
    "    pretty = ''.join(ch for ch in seq\n",
    "                     if ch not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END))\n",
    "    print(f\"{i:2d}: {pretty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: WORD-level\n",
      "Mean score: 4.962082362726267e-05\n",
      "Standard deviation of score: 0.000286735365135695\n"
     ]
    }
   ],
   "source": [
    "# evaluate a **word-based bigram model** on the test data\n",
    "# make sure to use the correct model!\n",
    "\n",
    "# score each line in the test data individually, then calculate the average score\n",
    "# you need not re-train your model, just be sure to use the correct model!\n",
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file(test_path)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from statistics import mean, stdev\n",
    "\n",
    "ngram   = 2\n",
    "by_char = False   \n",
    "train_path   = \"training_files/berp-training.txt\"\n",
    "train_tokens = lm.tokenize(lm.read_file(train_path),\n",
    "                           ngram, by_char=by_char)\n",
    "model = lm.LanguageModel(ngram)\n",
    "model.train(train_tokens)\n",
    "\n",
    "# score every sentence in the test set \n",
    "for line in test_data:\n",
    "    line = line.strip()\n",
    "    if not line:            \n",
    "        continue\n",
    "    sent_tokens = lm.tokenize_line(line, ngram, by_char=by_char)\n",
    "    scores.append(model.score(sent_tokens))\n",
    "\n",
    "μ = mean(scores)\n",
    "σ = stdev(scores)\n",
    "\n",
    "print(\"Tokenization: WORD-level\")\n",
    "print(f\"Mean score: {μ}\")\n",
    "print(f\"Standard deviation of score: {σ}\")\n",
    "# Expected runtime close to 0.1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 points\n",
    "\n",
    "# Find a new data set and run some training experiments\n",
    "# see if you can train your model on a data other than the provided data sets\n",
    "\n",
    "# Find another dataset on your own that fits following parameters and verify that you can run the language model on it for different values of n. \n",
    "# your file must:\n",
    "# - contain a minimum of 10,000 tokens (tokenized by \"words\") or 100,000 tokens (tokenized by characters)\n",
    "# - be in a text format\n",
    "# - not be restaurant reviews\n",
    "\n",
    "# your file need not (we encourage you to explore here):\n",
    "# - be in English\n",
    "\n",
    "# You must submit your data file to Gradescope along with `lm_model.py` and `ngram_lm.ipynb`.\n",
    "# if your data file is too large, you can use the `head` command to get a smaller version of it\n",
    "# please submit the smaller version of your data file *as well as* a zip file of the original data file\n",
    "\n",
    "# Here are some resources for finding datasets:\n",
    "# - https://www.gutenberg.org (public domain books)\n",
    "# - https://www.kaggle.com/datasets (you may have to manually convert your dataset into text.)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# First, print some attributes of your data set.\n",
    "# How big is it, how many unique tokens, etc.\n",
    "# Print an example text snippet from your data set.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Next, train your model on your data set for n = 1, 2, 3, 4, 5, ... 10\n",
    "# For each value of n, print the training time and generate three example sentences\n",
    "# stop if the training time exceeds 3 minutes, even if you haven't reached n = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14,533 raw lines from pride_prejudice.txt\n",
      "Total word tokens : 150,525\n",
      "Unique word types : 14,162\n",
      "\n",
      "--- sample snippet ---------------------------------------------\n",
      "that most will away with  her if they can. Though not in the least “impudent and\n",
      "mannish grown,”  she has no mere sensibility, no nasty niceness about her. The\n",
      "form of  passion common and likely to seem natural in Miss Austen’s day was so\n",
      "invariably connected with the display of one or the other, o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "===== n-gram experiments (word level) ==========================\n",
      "\n",
      "[n = 1] training time = 0.1 s, V = 6,144\n",
      "  1. \n",
      "  2. employ of of the off direction. than you\n",
      "  3. <UNK> to women, been\n",
      "\n",
      "[n = 2] training time = 0.1 s, V = 6,144\n",
      "  1. of the next day. and shoot as they were the most remarkable charm many of the parlour\n",
      "  2. [_copyright 1894 by their comparative height of some of\n",
      "  3. that his reading jane’s being quick, be sensible of it without the <UNK> me\n",
      "\n",
      "[n = 3] training time = 0.1 s, V = 6,144\n",
      "  1. thing, and as she spoke, joined to the enjoyment of his.\n",
      "  2. even miss austen has put into the\n",
      "  3. mr. bingley’s\n",
      "\n",
      "[n = 4] training time = 0.1 s, V = 6,144\n",
      "  1. “it ought to be\n",
      "  2. “your plan is a good <UNK> he replied, “say\n",
      "  3. the satisfaction of\n",
      "\n",
      "[n = 5] training time = 0.1 s, V = 6,144\n",
      "  1. jane then took it from her <UNK> and gave it to elizabeth. these\n",
      "  2. an agreement for <UNK> as may be <UNK> to himself and not\n",
      "  3. whatever manner he thought best; but her sister’s was involved in it, as\n",
      "\n",
      "[n = 6] training time = 0.2 s, V = 6,144\n",
      "  1. <UNK> he spoke well; but there were feelings besides those of the\n",
      "  2. her if they can. though not in the least <UNK> and <UNK> <UNK>\n",
      "  3. mrs. gardiner’s caution to elizabeth was punctually and kindly given on\n",
      "\n",
      "[n = 7] training time = 0.2 s, V = 6,144\n",
      "  1. of her manner as she spoke, joined to the circumstance itself, and many\n",
      "  2. who have been kept aloof by lydia’s <UNK>\n",
      "  3. jane will die of a broken heart, and then he will be sorry for what he\n",
      "\n",
      "[n = 8] training time = 0.2 s, V = 6,144\n",
      "  1. will be so numerous as to prevent your feeling the loss of the three of\n",
      "  2. thing, which for a time, and not <UNK> offended him. i could not\n",
      "  3. little design for a table, and i think it infinitely superior to miss\n",
      "\n",
      "[n = 9] training time = 0.2 s, V = 6,144\n",
      "  1. “i know them a little. their brother is a pleasant, gentlemanlike\n",
      "  2. “because honour, decorum, <UNK> <UNK> it. yes, miss\n",
      "  3. favourite child. at that moment she cared for no other. her younger\n",
      "\n",
      "[n = 10] training time = 0.2 s, V = 6,144\n",
      "  1. very greatly to my happiness; and, thirdly, which perhaps i ought to\n",
      "  2. everybody declared that he was the <UNK> young man in the world; and\n",
      "  3. at pemberley, before they left the country. miss darcy, though with a\n"
     ]
    }
   ],
   "source": [
    "import textwrap, os, random\n",
    "\n",
    "# Example: Project-Gutenberg\n",
    "data_path = \"pride_prejudice.txt\"   \n",
    "raw_lines = lm.read_file(data_path)            \n",
    "print(f\"Loaded {len(raw_lines):,} raw lines from {data_path}\")\n",
    "\n",
    "# Quick corpus stats\n",
    "word_tokens = lm.tokenize(raw_lines, ngram=1, by_char=False)\n",
    "unique_words = set(word_tokens)\n",
    "print(f\"Total word tokens : {len(word_tokens):,}\")\n",
    "print(f\"Unique word types : {len(unique_words):,}\")\n",
    "\n",
    "# show a random snippet (≈ 300 chars)\n",
    "joined_text = \" \".join(raw_lines)\n",
    "start = random.randrange(0, max(len(joined_text) - 300, 1))\n",
    "print(\"\\n--- sample snippet ---------------------------------------------\")\n",
    "print(textwrap.fill(joined_text[start : start + 300], width=80))\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Train & sample for n = 1 … 10\n",
    "print(\"\\n===== n-gram experiments (word level) ==========================\")\n",
    "\n",
    "for n in range(1, 11):\n",
    "    t0 = time.time()\n",
    "    tokens = lm.tokenize(raw_lines, ngram=n, by_char=False)   \n",
    "    model = lm.LanguageModel(n)\n",
    "    model.train(tokens)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\n[n = {n}] training time = {elapsed:0.1f} s, V = {len(model.vocab):,}\")\n",
    "\n",
    "    # generate 3 example sentences\n",
    "    for i in range(1, 4):\n",
    "        sent = model.generate_sentence()\n",
    "        pretty = \" \".join(tok for tok in sent\n",
    "                          if tok not in (lm.SENTENCE_BEGIN, lm.SENTENCE_END))\n",
    "        print(f\"  {i}. {pretty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP!!!\n",
    "=====\n",
    "\n",
    "Before turning any homework notebook in:\n",
    "\n",
    "- When you have finished each notebook, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that your notebook displays properly in Gradescope\n",
    "- Double check that your notebook does not display too much output (don't make us go on a treasure hunt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
